# Prompt Libraries & Datasets Repository

This repository is dedicated to tracking prompt libraries and datasets related to prompt engineering, AI, and NLP. Use this as a living document to catalog useful resources as you discover them.

---

## üìö Prompt Libraries

A curated list of prompt libraries, frameworks, and tools:

- [Wharton School of Business Generative AI Labs Prompt Library](https://hd3ns092ns.notion.site/1b3dc3333315802a9e99cafedb321048?v=1b3dc3333315804693e2000c7ca70b7b): A curated collection of generative AI prompts and resources from the Wharton School of Business.<br><br>

---

## üìä Dataset Collections

Curated collections of datasets for LLM training and research:

- [LLMDataHub: Awesome Datasets for LLM Training](https://github.com/Zjh-819/LLMDataHub): A continuously updated repository collecting high-quality, large-scale corpora for LLM training, including chatbot-specific datasets with details on size, language, and usage. Ideal for researchers and practitioners seeking resources for training and improving LLMs.<br><br>
- [LLM Training Datasets (Hugging Face Collection)](https://huggingface.co/collections/sugatoray/llm-training-datasets-65dbe4ab2b0037ec198b09ab): A curated collection of datasets for training large language models, hosted on Hugging Face.<br><br>

---

## üìä Datasets

A collection of individual datasets useful for prompt engineering, training, and evaluation:

- [Common Pile v0.1](https://huggingface.co/collections/common-pile/common-pile-v01-68307d37df48e36f02717f21): All resources related to Common Pile v0.1, an 8TB dataset of public domain and openly licensed text.<br><br>
- [Common Corpus](https://huggingface.co/datasets/PleIAs/common_corpus): The largest open and permissible licensed text dataset (2 trillion tokens) including books, newspapers, scientific articles, government/legal documents, code, and more. Created by Pleias and partners for the Current AI initiative.<br><br>
- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb): The üç∑ FineWeb dataset consists of more than 15T tokens of cleaned and deduplicated English web data from CommonCrawl. Optimized for LLM performance and processed with the [datatrove library](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py).<br><br>
- [CulturaX](https://huggingface.co/datasets/uonlp/CulturaX): Cleaned, enormous, and public multilingual dataset with 6.3 trillion tokens in 167 languages. CulturaX is meticulously cleaned and deduplicated for LLM development, supporting research and advancements in multilingual LLMs.<br><br>
- [SpeakerVid-5M](https://dorniwang.github.io/SpeakerVid-5M/): A large-scale, high-quality dataset for audio-visual dyadic interactive human generation with over 8,743 hours and 5.2 million video clips. Designed for virtual human tasks including dialogue, listening, and conversation scenarios.<br><br>
- [EmbRACE-3K](https://mxllc.github.io/EmbRACE-3K/): A dataset of over 3,000 language-guided embodied tasks in photorealistic environments with 26,000 decision steps. Designed for evaluating embodied reasoning capabilities including navigation, object manipulation, and multi-stage goal execution.<br><br>
- [Caselaw Access Project](https://huggingface.co/datasets/common-pile/caselaw_access_project): A comprehensive legal dataset containing 6.7 million U.S. federal and state court cases from the last 365 years. Includes nearly 40 million pages of court decisions and judges' opinions from sources like Harvard Law Library and the Law Library of Congress.<br><br>
