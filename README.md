# Prompt Libraries & Datasets Repository

This repository is dedicated to tracking prompt libraries and datasets related to prompt engineering, AI, and NLP. Use this as a living document to catalog useful resources as you discover them.

---

## üìö Prompt Libraries

A curated list of prompt libraries, frameworks, and tools:

- [Wharton School of Business Generative AI Labs Prompt Library](https://hd3ns092ns.notion.site/1b3dc3333315802a9e99cafedb321048?v=1b3dc3333315804693e2000c7ca70b7b): A curated collection of generative AI prompts and resources from the Wharton School of Business.<br><br>
- [GitHub Spark System Prompt](https://github.com/simonw/system-exploration-g/blob/main/src/system_prompt.md): A comprehensive system prompt for GitHub Spark, providing insights into prompt engineering for code generation and development assistance tools.<br><br>

---

## üìä Dataset Collections

Curated collections of datasets for LLM training and research:

- [LLMDataHub: Awesome Datasets for LLM Training](https://github.com/Zjh-819/LLMDataHub): A continuously updated repository collecting high-quality, large-scale corpora for LLM training, including chatbot-specific datasets with details on size, language, and usage. Ideal for researchers and practitioners seeking resources for training and improving LLMs.<br><br>
- [LLM Training Datasets (Hugging Face Collection)](https://huggingface.co/collections/sugatoray/llm-training-datasets-65dbe4ab2b0037ec198b09ab): A curated collection of datasets for training large language models, hosted on Hugging Face.<br><br>

---

## üìä Datasets

A collection of individual datasets useful for prompt engineering, training, and evaluation:

- [Common Pile v0.1](https://huggingface.co/collections/common-pile/common-pile-v01-68307d37df48e36f02717f21): All resources related to Common Pile v0.1, an 8TB dataset of public domain and openly licensed text.<br><br>
- [Common Corpus](https://huggingface.co/datasets/PleIAs/common_corpus): The largest open and permissible licensed text dataset (2 trillion tokens) including books, newspapers, scientific articles, government/legal documents, code, and more. Created by Pleias and partners for the Current AI initiative.<br><br>
- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb): The üç∑ FineWeb dataset consists of more than 15T tokens of cleaned and deduplicated English web data from CommonCrawl. Optimized for LLM performance and processed with the [datatrove library](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py).<br><br>
- [CulturaX](https://huggingface.co/datasets/uonlp/CulturaX): Cleaned, enormous, and public multilingual dataset with 6.3 trillion tokens in 167 languages. CulturaX is meticulously cleaned and deduplicated for LLM development, supporting research and advancements in multilingual LLMs.<br><br>
- [SpeakerVid-5M](https://dorniwang.github.io/SpeakerVid-5M/): A large-scale, high-quality dataset for audio-visual dyadic interactive human generation with over 8,743 hours and 5.2 million video clips. Designed for virtual human tasks including dialogue, listening, and conversation scenarios.<br><br>
- [EmbRACE-3K](https://mxllc.github.io/EmbRACE-3K/): A dataset of over 3,000 language-guided embodied tasks in photorealistic environments with 26,000 decision steps. Designed for evaluating embodied reasoning capabilities including navigation, object manipulation, and multi-stage goal execution.<br><br>
- [Caselaw Access Project](https://huggingface.co/datasets/common-pile/caselaw_access_project): A comprehensive legal dataset containing 6.7 million U.S. federal and state court cases from the last 365 years. Includes nearly 40 million pages of court decisions and judges' opinions from sources like Harvard Law Library and the Law Library of Congress.<br><br>
- [ZAPBench](https://github.com/google-research/zapbench): A whole-brain activity dataset from larval zebrafish featuring recordings of roughly 70,000 neurons with comprehensive connectome mapping. Designed for building and benchmarking predictive models of neural activity patterns across an entire vertebrate brain.<br><br>
- [MegaScience](https://github.com/GAIR-NLP/MegaScience): A large-scale mixture of high-quality scientific reasoning datasets totaling 1.25 million instances across 7 scientific disciplines. Features TextbookReasoning with 650k questions from university-level textbooks and comprehensive evaluation across 15 benchmarks.<br><br>
- [Zebra-CoT](https://github.com/multimodal-reasoning-lab/Bagel-Zebra-CoT): A diverse dataset with 182,384 samples for interleaved vision-language reasoning, featuring logically coherent text-image reasoning traces. Covers scientific questions, 2D/3D visual reasoning, visual logic problems, and strategic games like chess.<br><br>
- [VideoMind](https://github.com/cdx-cindy/VideoMind): A video-centric omni-modal dataset with 103K video samples featuring hierarchical textual descriptions across factual, abstract, and intent layers. Contains over 22 million words with Chain-of-Thought generation for deep-cognitive video understanding and intent recognition.<br><br>
