# Prompt Libraries & Datasets Repository

This repository is dedicated to tracking prompt libraries and datasets related to prompt engineering, AI, and NLP. Use this as a living document to catalog useful resources as you discover them.

---

## üìö Prompt Libraries

A curated list of prompt libraries, frameworks, and tools:

- [Wharton School of Business Generative AI Labs Prompt Library](https://hd3ns092ns.notion.site/1b3dc3333315802a9e99cafedb321048?v=1b3dc3333315804693e2000c7ca70b7b): A curated collection of generative AI prompts and resources from the Wharton School of Business.<br><br>

---

## üìä Dataset Collections

Curated collections of datasets for LLM training and research:

- [LLMDataHub: Awesome Datasets for LLM Training](https://github.com/Zjh-819/LLMDataHub): A continuously updated repository collecting high-quality, large-scale corpora for LLM training, including chatbot-specific datasets with details on size, language, and usage. Ideal for researchers and practitioners seeking resources for training and improving LLMs.<br><br>
- [LLM Training Datasets (Hugging Face Collection)](https://huggingface.co/collections/sugatoray/llm-training-datasets-65dbe4ab2b0037ec198b09ab): A curated collection of datasets for training large language models, hosted on Hugging Face.<br><br>

---

## üìä Datasets

A collection of individual datasets useful for prompt engineering, training, and evaluation:

- [Common Pile v0.1](https://huggingface.co/collections/common-pile/common-pile-v01-68307d37df48e36f02717f21): All resources related to Common Pile v0.1, an 8TB dataset of public domain and openly licensed text.<br><br>
- [Common Corpus](https://huggingface.co/datasets/PleIAs/common_corpus): The largest open and permissible licensed text dataset (2 trillion tokens) including books, newspapers, scientific articles, government/legal documents, code, and more. Created by Pleias and partners for the Current AI initiative.<br><br>
- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb): The üç∑ FineWeb dataset consists of more than 15T tokens of cleaned and deduplicated English web data from CommonCrawl. Optimized for LLM performance and processed with the [datatrove library](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py).


